{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine tune small bert",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdvtxSxx0YMr",
        "outputId": "81402451-d470-47dc-a00a-c98491aa386f"
      },
      "source": [
        "!pip install -q tf-models-official==2.3.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10kB 29.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 17.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 14.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40kB 14.6MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71kB 11.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 81kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 92kB 11.9MB/s eta 0:00:01\r\u001b[K     |████                            | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 174kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 184kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 194kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 204kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 215kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 225kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 235kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 245kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 256kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 266kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 276kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 286kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 296kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 307kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 317kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 327kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 337kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 348kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 358kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 368kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 378kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 389kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 399kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 409kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 419kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 430kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 440kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 450kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 460kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 471kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 481kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 491kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 501kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 512kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 522kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 532kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 542kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 552kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 563kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 573kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 583kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 593kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 604kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 614kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 624kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 634kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 645kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 655kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 665kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 675kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 686kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 696kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 706kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 716kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 727kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 737kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 747kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 757kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 768kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 778kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 788kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 798kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 808kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 819kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 829kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 839kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 849kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 30.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.7MB 86kB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.8MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnv-8HJzpo7",
        "outputId": "2e195552-80a1-449d-be94-2fa5d6f9da9b"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "tfds.disable_progress_bar()\r\n",
        "\r\n",
        "from official.modeling import tf_utils\r\n",
        "from official import nlp\r\n",
        "from official.nlp import bert\r\n",
        "\r\n",
        "# Load the required submodules\r\n",
        "import official.nlp.optimization\r\n",
        "import official.nlp.bert.bert_models\r\n",
        "import official.nlp.bert.configs\r\n",
        "import official.nlp.bert.run_classifier\r\n",
        "import official.nlp.bert.tokenization\r\n",
        "import official.nlp.data.classifier_data_lib\r\n",
        "import official.nlp.modeling.losses\r\n",
        "import official.nlp.modeling.models\r\n",
        "import official.nlp.modeling.networks\r\n",
        "import json\r\n",
        "\r\n",
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\r\n",
        "\r\n",
        "#preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\")\r\n",
        "#print(\"preprocessor loaded\", preprocessor)\r\n",
        "#encoder_inputs = preprocessor(text_input)\r\n",
        "\r\n",
        "glue, info = tfds.load('glue/mrpc', with_info=True,\r\n",
        "                       # It's small, load the whole dataset\r\n",
        "                       batch_size=-1)\r\n",
        "list(glue.keys())\r\n",
        "glue_train = glue['train']\r\n",
        "glue_train['label'] = tf.constant(np.insert(glue_train['label'], 0, 0, axis=0)) \r\n",
        "glue_train['sentence1'] = tf.constant(np.insert(glue_train['sentence1'], 0, \"I like playing football\", axis=0))\r\n",
        "glue_train['sentence2'] = tf.constant(np.insert(glue_train['sentence2'], 0, \"Playing football has never been my hobby\", axis=0))\r\n",
        "\r\n",
        "glue_train['label'] = tf.constant(np.insert(glue_train['label'], 0, 1, axis=0)) \r\n",
        "glue_train['sentence1'] = tf.constant(np.insert(glue_train['sentence1'], 0, \"Working hard offer a well chance to succeed\", axis=0))\r\n",
        "glue_train['sentence2'] = tf.constant(np.insert(glue_train['sentence2'], 0, \"You have a good chance to win, if you work seriously\", axis=0))\r\n",
        "\r\n",
        "for key, value in glue_train.items():\r\n",
        "  print(f\"{key:9s}: {value[0].numpy()}\")\r\n",
        "\r\n",
        "tokenizer = bert.tokenization.FullTokenizer(\r\n",
        "     vocab_file=\"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12/vocab.txt\",\r\n",
        "     do_lower_case=True)\r\n",
        "\r\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\r\n",
        "\r\n",
        "max_seq_length =256\r\n",
        "def encode_sentence(s, tokenizer):\r\n",
        "    tokens = list(tokenizer.tokenize(s))\r\n",
        "    tokens.append('[SEP]')\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\r\n",
        " #bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\")\r\n",
        "def bert_encode(glue_dict, tokenizer):\r\n",
        "    sentence1 = tf.ragged.constant([\r\n",
        "        encode_sentence(s, tokenizer)\r\n",
        "        for s in np.array(glue_dict[\"sentence1\"])])\r\n",
        "    sentence2 = tf.ragged.constant([\r\n",
        "        encode_sentence(s, tokenizer)\r\n",
        "        for s in np.array(glue_dict[\"sentence2\"])])\r\n",
        "\r\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\r\n",
        "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\r\n",
        "    input_word_ids = tf.stack([tf.pad(e, [[0, max_seq_length - len(e.numpy())]]) for e in input_word_ids], axis=0)\r\n",
        "\r\n",
        "    input_mask = tf.ones_like(input_word_ids)\r\n",
        "\r\n",
        "    type_cls = tf.zeros_like(cls)\r\n",
        "    type_s1 = tf.zeros_like(sentence1)\r\n",
        "    type_s2 = tf.ones_like(sentence2)\r\n",
        "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1)\r\n",
        "    input_type_ids = tf.stack([tf.pad(e, [[0, max_seq_length - len(e.numpy())]]) for e in input_type_ids], axis=0)\r\n",
        "    inputs = {\r\n",
        "        'input_word_ids': input_word_ids,\r\n",
        "        'input_mask': input_mask,\r\n",
        "        'input_type_ids': input_type_ids}\r\n",
        "    #for key, value in inputs.items():\r\n",
        "    #    print(f'{key:15s} shape: {value.shape}')\r\n",
        "    return inputs\r\n",
        "\r\n",
        "glue_train = bert_encode(glue['train'], tokenizer)\r\n",
        "glue_train_labels = glue['train']['label']\r\n",
        "\r\n",
        "glue_validation = bert_encode(glue['validation'], tokenizer)\r\n",
        "glue_validation_labels = glue['validation']['label']\r\n",
        "\r\n",
        "glue_test = bert_encode(glue['test'], tokenizer)\r\n",
        "glue_test_labels = glue['test']['label']\r\n",
        "\r\n",
        "#config_dict = json.loads(tf.io.gfile.GFile(\"assets/bert_config.json\").read())\r\n",
        "#bert_config = bert.configs.BertConfig.from_dict(config_dict)\r\n",
        "\r\n",
        "#bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx      : 1680\n",
            "label    : 1\n",
            "sentence1: b'Working hard offer a well chance to succeed'\n",
            "sentence2: b'You have a good chance to win, if you work seriously'\n",
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiCbfEOo7Soy",
        "outputId": "33316cac-26bd-4d61-d200-a0de145292b8"
      },
      "source": [
        "def build_classifier_model(num_classes):\r\n",
        "    inputs = dict(\r\n",
        "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\r\n",
        "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\r\n",
        "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\r\n",
        "    )\r\n",
        "\r\n",
        "    #encoder = bert.bert_models.get_transformer_encoder(bert_config, max_seq_length, output_range=1)\r\n",
        "    #inputs = encoder.inputs\r\n",
        "    #_, net = encoder(inputs)\r\n",
        "\r\n",
        "    encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1\", trainable=True, name='encoder')\r\n",
        "    net = encoder(inputs)['pooled_output']\r\n",
        "    net = tf.keras.layers.Dropout(rate=0.1)(net)\r\n",
        "    net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\r\n",
        "    return tf.keras.Model(inputs, net, name='prediction')\r\n",
        "\r\n",
        "bert_classifier = build_classifier_model(2)\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "batch_size = 32\r\n",
        "eval_batch_size = batch_size\r\n",
        "\r\n",
        "train_data_size = len(glue_train_labels)\r\n",
        "steps_per_epoch = int(train_data_size / batch_size)\r\n",
        "num_train_steps = steps_per_epoch * epochs\r\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\r\n",
        "\r\n",
        "# creates an optimizer with learning rate schedule\r\n",
        "optimizer = nlp.optimization.create_optimizer(\r\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\r\n",
        "\r\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "bert_classifier.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n",
        "export_dir='./myburt-md'\r\n",
        "\r\n",
        "#bert_classifier = tf.saved_model.load(export_dir)\r\n",
        "\r\n",
        "tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=64)\r\n",
        "\r\n",
        "bert_classifier.fit(\r\n",
        "      glue_train, glue_train_labels,\r\n",
        "      validation_data=(glue_validation, glue_validation_labels),\r\n",
        "      batch_size=batch_size,\r\n",
        "      epochs=epochs)\r\n",
        "\r\n",
        "#tf.saved_model.save(bert_classifier, export_dir)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 60s 410ms/step - loss: 0.6709 - accuracy: 0.6660 - val_loss: 0.5515 - val_accuracy: 0.7132\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 47s 412ms/step - loss: 0.5889 - accuracy: 0.6894 - val_loss: 0.5316 - val_accuracy: 0.7230\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 48s 419ms/step - loss: 0.5218 - accuracy: 0.7539 - val_loss: 0.5053 - val_accuracy: 0.7647\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 49s 425ms/step - loss: 0.4432 - accuracy: 0.8014 - val_loss: 0.4701 - val_accuracy: 0.7941\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 50s 432ms/step - loss: 0.4175 - accuracy: 0.8253 - val_loss: 0.5152 - val_accuracy: 0.7819\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 50s 434ms/step - loss: 0.3749 - accuracy: 0.8486 - val_loss: 0.5647 - val_accuracy: 0.7868\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 50s 436ms/step - loss: 0.3435 - accuracy: 0.8607 - val_loss: 0.5427 - val_accuracy: 0.7868\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 50s 437ms/step - loss: 0.3088 - accuracy: 0.8829 - val_loss: 0.5769 - val_accuracy: 0.7917\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 50s 435ms/step - loss: 0.2792 - accuracy: 0.8912 - val_loss: 0.5662 - val_accuracy: 0.7868\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 50s 435ms/step - loss: 0.2725 - accuracy: 0.8961 - val_loss: 0.5818 - val_accuracy: 0.7843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4d70f9f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpWEue2t7D3e",
        "outputId": "f11ddf6d-334f-468e-a0b7-906ab6799073"
      },
      "source": [
        "glue_example = {\r\n",
        "        'sentence1': [\r\n",
        "            'The rain in Spain falls mainly on the plain.',\r\n",
        "            'The rain in Spain falls mainly on the plain.',\r\n",
        "            'After i run, i feel good',\r\n",
        "            'After i run, i feel good',\r\n",
        "            'Look I fine tuned BERT.'],\r\n",
        "        'sentence2': [\r\n",
        "            'Rains never falls on the plain in spain.',\r\n",
        "            'It mostly rains on the flat lands of Spain.',\r\n",
        "            'I feel hungry when i run',\r\n",
        "            'I feel well when i run',\r\n",
        "            'Is it working? This does not match.'],\r\n",
        "        'labels': [0, 1, 0, 1, 0]\r\n",
        "    }\r\n",
        "my_examples = bert_encode(\r\n",
        "    glue_dict=glue_example,\r\n",
        "    tokenizer=tokenizer)\r\n",
        "\r\n",
        "result = tf.math.softmax(bert_classifier(my_examples, training=False))\r\n",
        "result = tf.argmax(input=result, axis=1)\r\n",
        "for i in range(len(result)):\r\n",
        "    print(result[i], glue_example['labels'][i])\r\n",
        "\r\n",
        "glue_batch = {key: val[:10] for key, val in glue_validation.items()}\r\n",
        "\r\n",
        "result = tf.math.softmax(bert_classifier(\r\n",
        "    glue_batch, training=True\r\n",
        ")).numpy()\r\n",
        "result = tf.argmax(input=result, axis=1)\r\n",
        "for i in range(len(result)):\r\n",
        "    print(glue['validation']['sentence1'].numpy()[i])\r\n",
        "    print(glue['validation']['sentence2'].numpy()[i])\r\n",
        "    print(result[i], glue_validation_labels.numpy()[i])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int64) 0\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "tf.Tensor(0, shape=(), dtype=int64) 0\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "tf.Tensor(0, shape=(), dtype=int64) 0\n",
            "b\"The show 's closure affected third-quarter earnings per share by a penny .\"\n",
            "b'The company said this impacted earnings by a penny a share .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b'Terri Schiavo , 39 , underwent the procedure at the Tampa Bay area hospice where she has been living for several years , said her father , Bob Schindler .'\n",
            "b'The tube was removed Wednesday from Terri Schiavo , 39 , at the Tampa Bay-area hospice where she has lived for several years .'\n",
            "tf.Tensor(0, shape=(), dtype=int64) 1\n",
            "b'The study , published Monday in the journal Molecular Brain Research , is likely to also apply to humans , its authors said .'\n",
            "b'The study , conducted on the brains of developing mice , was being published today in the journal Molecular Brain Research .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 0\n",
            "b'It also offers a built-in NAND flash boot loader so that high-density NAND flash memory can be used without having to install an additional support chip .'\n",
            "b'The S3C2440 has a built-in NAND flash boot loader , for example , so that high-density NAND flash memory can be installed without an additional support chip .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b\"Dr Mark McClean , Jonathan 's family doctor , said if the drug had been administered earlier Jonathan would have retained more of his brain functions .\"\n",
            "b\"Dr Mark McClean , the family 's GP , said had the drug been administered to Jonathan earlier , he would have retained more of his brain function .\"\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b'\" There were more people surrounding the clubhouse than the Unabomber \\'s house up in the hills , \" Baker said .'\n",
            "b'\" There are more people surrounding the clubhouse than surrounded the Unabomber \\'s home in the hills .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b'The announcement was made during the recording of a Christmas concert attended by top Vatican cardinals , bishops , and many elite from Italian society , witnesses said .'\n",
            "b'The broadside came during the recording on Saturday night of a Christmas concert attended by top Vatican cardinals , bishops and many elite of Italian society , witnesses said .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b'The worm attacks Windows computers via a hole in the operating system , an issue Microsoft on July 16 had warned about .'\n",
            "b'The worm attacks Windows computers via a hole in the operating system , which Microsoft warned of 16 July .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n",
            "b'Mr. Kerkorian tried unsuccessfully to take over Chrysler in 1995 , but did win representation on its board .'\n",
            "b'Kerkorian and Tracinda had also tried to take over Chrysler in 1995 .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 0\n",
            "b'Witnesses said they believed the man planned to crash the Launceston-bound Qantas flight 1737 , which was carrying 47 passengers and six crew .'\n",
            "b'Witnesses believe he wanted to crash Flight 1737 , which had 47 passengers and six crew .'\n",
            "tf.Tensor(1, shape=(), dtype=int64) 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}