{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of states and number of actions\n",
    "n_states = 3\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial state distribution\n",
    "mu = np.array([[1, 0, 0]]).T\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the upper bound coefficients for the action A\n",
    "# define the reward matrix for action A\n",
    "R_A = np.zeros((n_states, 1), np.float)\n",
    "R_A[0, 0] = 1\n",
    "R_A[1, 0] = 0\n",
    "R_A[2, 0] = 0\n",
    "R_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the transition matrix for action A\n",
    "P_A = np.zeros((n_states, n_states), np.float)\n",
    "P_A[0, 1] = 1\n",
    "P_A[1, 0] = 1\n",
    "P_A[2, 1] = 1\n",
    "P_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0.9,  0. ],\n",
       "       [ 0.9, -1. ,  0. ],\n",
       "       [ 0. ,  0.9, -1. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper bound A matrix for action A\n",
    "A_up_A = gamma * P_A - np.eye(3,3)\n",
    "A_up_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same for action B\n",
    "# define the reward matrix for action B\n",
    "R_B = np.zeros((n_states, 1), np.float)\n",
    "R_B[0, 0] = 10\n",
    "R_B[1, 0] = 1\n",
    "R_B[2, 0] = 10\n",
    "R_B\n",
    "# Define the transition matrix for action A\n",
    "P_B = np.zeros((n_states, n_states), np.float)\n",
    "P_B[0, 2] = 1\n",
    "P_B[1, 2] = 1\n",
    "P_B[2, 2] = 1\n",
    "P_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0. ,  0.9],\n",
       "       [ 0. , -1. ,  0.9],\n",
       "       [ 0. ,  0. , -0.1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper bound A matrix for action B\n",
    "A_up_B = gamma * P_B - np.eye(3,3)\n",
    "A_up_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper bound matrix for all actions and all states\n",
    "A_up = np.vstack((A_up_A, A_up_B))\n",
    "# verify the shape: number of constraints are equal to |actions| * |states|\n",
    "assert(A_up.shape[0] == n_states * n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward vector is obtained by stacking the two vectors\n",
    "R = np.vstack((R_A, R_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mu\n",
    "b_up = -R\n",
    "# Solve the linear program\n",
    "res = scipy.optimize.linprog(c, A_up, b_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results: state values\n",
    "V_ = res.x\n",
    "V_\n",
    "V = V_.reshape((-1, 1))\n",
    "V\n",
    "np.savetxt(\"solution/V.txt\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transition matrix. On the rows we have states and actions, on the columns we have next states\n",
    "P = np.vstack((P_A, P_B))\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the action value formula to calculate the action values for each state action pair.\n",
    "Q_sa = R + gamma * P.dot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88.32127683],\n",
       "       [ 89.99999645],\n",
       "       [ 87.32127683],\n",
       "       [100.00000622],\n",
       "       [ 91.00000622],\n",
       "       [100.00000622]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first three rows are associated to action A, the last three are associated to action B\n",
    "Q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88.32127683, 100.00000622],\n",
       "       [ 89.99999645,  91.00000622],\n",
       "       [ 87.32127683, 100.00000622]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sa_2 = np.stack((Q_sa[:3, 0], Q_sa[3:, 0]), axis=1)\n",
    "Q_sa_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions = np.reshape(np.argmax(Q_sa_2, axis=1), (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
