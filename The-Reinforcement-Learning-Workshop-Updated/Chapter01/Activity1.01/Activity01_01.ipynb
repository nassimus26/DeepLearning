{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abstract class representing the agent\n",
    "Init with the action space and the function pi returning the action\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, action_space: gym.spaces.Space):\n",
    "        \"\"\"\n",
    "        Constructor of the agent class.\n",
    "        \n",
    "        Args:\n",
    "            action_space (gym.spaces.Space): environment action space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This class cannot be instantiated.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def pi(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Agent's policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): environment state\n",
    "        \n",
    "        Returns:\n",
    "            The selected action\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousAgent(Agent):\n",
    "    def __init__(self, action_space: gym.spaces.Space, seed=46):\n",
    "        # setup seed\n",
    "        np.random.seed(seed)\n",
    "        # check the action space type\n",
    "        if not isinstance(action_space, gym.spaces.Box):\n",
    "            raise ValueError(\"This is a Continuous Agent pass as input a Box Space.\")\n",
    "\n",
    "        # initialize the distribution according to the action space type\n",
    "        if (action_space.low == -np.inf) and (action_space.high == np.inf):\n",
    "            # the distribution is a normal distribution\n",
    "            self._pi = lambda: np.random.normal(loc=0, scale=1, size=action_space.shape)\n",
    "            return\n",
    "        if (action_space.low != -np.inf) and (action_space.high != np.inf):\n",
    "            # the distribution is a uniform distribution\n",
    "            self._pi = lambda: np.random.uniform(\n",
    "                low=action_space.low, high=action_space.high, size=action_space.shape\n",
    "            )\n",
    "            return\n",
    "        if action_space.low == -np.inf:\n",
    "            # negative exponential distribution\n",
    "            self._pi = (\n",
    "                lambda: -np.random.exponential(size=action_space.shape)\n",
    "                + action_space.high\n",
    "            )\n",
    "            return\n",
    "        if action_space.high == np.inf:\n",
    "            # exponential distribution\n",
    "            self._pi = (\n",
    "                lambda: np.random.exponential(size=action_space.shape)\n",
    "                + action_space.low\n",
    "            )\n",
    "            return\n",
    "\n",
    "    def pi(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Policy: simply call the internal _pi().\n",
    "        \n",
    "        This is a random agent so the action is independent from the observation.\n",
    "        For real agents the action depends on the observation.\n",
    "        \"\"\"\n",
    "        return self._pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteAgent(Agent):\n",
    "    def __init__(self, action_space: gym.spaces.Space, seed=46):\n",
    "        # setup seed\n",
    "        np.random.seed(seed)\n",
    "        # check the action space type\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError(\"This is a Discrete Agent pass as input a Discrete Space.\")\n",
    "\n",
    "        # initialize the distribution according to the action space n attribute\n",
    "        # the distribution is a uniform distribution\n",
    "        self._pi = lambda: np.random.randint(low=0, high=action_space.n)\n",
    "\n",
    "    def pi(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Policy: simply call the internal _pi().\n",
    "        \n",
    "        This is a random agent so the action is independent from the observation.\n",
    "        For real agents the action depends on the observation.\n",
    "        \"\"\"\n",
    "        return self._pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent(action_space: gym.spaces.Space, seed=46):\n",
    "    \"\"\"\n",
    "    Returns the correct agent based on the action space type\n",
    "    \"\"\"\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        return DiscreteAgent(action_space, seed)\n",
    "    if isinstance(action_space, gym.spaces.Box):\n",
    "        return ContinuousAgent(action_space, seed)\n",
    "    raise ValueError(\n",
    "        \"Only Box spaces or Discrete Spaces are allowed, check the action space of the environment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Name\n",
    "env_name = \"CartPole-v0\"\n",
    "# Number of episodes\n",
    "episodes = 10\n",
    "# Number of Timesteps of each episodes\n",
    "timesteps = 100\n",
    "# Discount factor\n",
    "gamma = 1.0\n",
    "# seed environment\n",
    "seed = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to show the environment in a notebook\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 0, Timesteps: 27, Return: 28.0\n",
      "Episode Number: 1, Timesteps: 9, Return: 10.0\n",
      "Episode Number: 2, Timesteps: 13, Return: 14.0\n",
      "Episode Number: 3, Timesteps: 16, Return: 17.0\n",
      "Episode Number: 4, Timesteps: 31, Return: 32.0\n",
      "Episode Number: 5, Timesteps: 10, Return: 11.0\n",
      "Episode Number: 6, Timesteps: 14, Return: 15.0\n",
      "Episode Number: 7, Timesteps: 11, Return: 12.0\n",
      "Episode Number: 8, Timesteps: 10, Return: 11.0\n",
      "Episode Number: 9, Timesteps: 30, Return: 31.0\n",
      "Statistics on Return: Average: 18.1, Variance: 68.89000000000001\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env.seed(seed)\n",
    "# the last argument is needed to record all episodes\n",
    "# otherwise gym would record only some of them\n",
    "# The monitor saves the episodes inside the folder ./gym-results\n",
    "env = wrappers.Monitor(\n",
    "    env, \"./gym-results\", force=True, video_callable=lambda episode_id: True\n",
    ")\n",
    "\n",
    "agent = make_agent(env.action_space, seed)\n",
    "\n",
    "# list of returns\n",
    "episode_returns = []\n",
    "\n",
    "# loop for the episodes\n",
    "for episode_number in range(episodes):\n",
    "    # here we are inside an episode\n",
    "\n",
    "    # reset cumulated gamma\n",
    "    gamma_cum = 1\n",
    "\n",
    "    # return of the current episode\n",
    "    episode_return = 0\n",
    "\n",
    "    # the reset function resets the environment and returns\n",
    "    # the first environment observation\n",
    "    observation = env.reset()\n",
    "\n",
    "    # loop for the given number of timesteps or\n",
    "    # until the episode is terminated\n",
    "    for timestep_number in range(timesteps):\n",
    "\n",
    "        # render the environment\n",
    "        # env.render()\n",
    "\n",
    "        # select the action\n",
    "        action = agent.pi(observation)\n",
    "\n",
    "        # apply the selected action by calling env.step\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # increment the return\n",
    "        episode_return += reward * gamma_cum\n",
    "\n",
    "        # update the value of cumulated discount factor\n",
    "        gamma_cum = gamma_cum * gamma\n",
    "\n",
    "        # if done the episode is terminated, we have to reset\n",
    "        # the environment\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Episode Number: {episode_number}, Timesteps: {timestep_number}, Return: {episode_return}\"\n",
    "            )\n",
    "            # break from the timestep loop\n",
    "            break\n",
    "\n",
    "    episode_returns.append(episode_return)\n",
    "\n",
    "# close the environment\n",
    "env.close()\n",
    "\n",
    "# Calculate return statistics\n",
    "avg_return = np.mean(episode_returns)\n",
    "std_return = np.std(episode_returns)\n",
    "var_return = std_return ** 2  # variance is std^2\n",
    "\n",
    "print(f\"Statistics on Return: Average: {avg_return}, Variance: {var_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render the episodes\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "episodes_to_watch = 1\n",
    "for episode in range(episodes_to_watch):\n",
    "    video = io.open(\n",
    "        f\"./gym-results/openaigym.video.{env.file_infix}.video{episode:06d}.mp4\", \"r+b\"\n",
    "    ).read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    display(\n",
    "        HTML(\n",
    "            data=\"\"\"\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>\"\"\".format(\n",
    "                encoded.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}