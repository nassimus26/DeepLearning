{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! convert -delay X gym-results/openaigym.video.0.5889.video000000.mp4 -loop 0 cartpole.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gif/cartpole.gif\" alt=\"CartPole\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 08:13:40.845761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\cd_bo\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 192, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\cd_bo\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\work\\openai\\baselines\\baselines\\run.py\", line 12, in <module>\n",
      "    from baselines.common.cmd_util import common_arg_parser, parse_unknown_args, make_vec_env, make_env\n",
      "  File \"c:\\work\\openai\\baselines\\baselines\\common\\cmd_util.py\", line 12, in <module>\n",
      "    from gym.wrappers import FlattenDictWrapper\n",
      "ImportError: cannot import name 'FlattenDictWrapper' from 'gym.wrappers' (C:\\work\\vpython\\lib\\site-packages\\gym\\wrappers\\__init__.py)\n"
     ]
    }
   ],
   "source": [
    "! python -m baselines.run --alg=deepq --env=CartPole-v0 --save_path=./cartpole_model.pkl --num_timesteps=1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 08:13:45.598700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\cd_bo\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 192, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\cd_bo\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\work\\openai\\baselines\\baselines\\run.py\", line 12, in <module>\n",
      "    from baselines.common.cmd_util import common_arg_parser, parse_unknown_args, make_vec_env, make_env\n",
      "  File \"c:\\work\\openai\\baselines\\baselines\\common\\cmd_util.py\", line 12, in <module>\n",
      "    from gym.wrappers import FlattenDictWrapper\n",
      "ImportError: cannot import name 'FlattenDictWrapper' from 'gym.wrappers' (C:\\work\\vpython\\lib\\site-packages\\gym\\wrappers\\__init__.py)\n"
     ]
    }
   ],
   "source": [
    "# Load the model saved in cartpole_model.pkl and visualize the learned policy\n",
    "! python -m baselines.run --alg=deepq --env=CartPole-v0 --load_path=./cartpole_model.pkl --num_timesteps=0 --play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "# Import the desired algorithm from baselines\n",
    "from baselines import deepq\n",
    "from baselines import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\work\\vpython\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "C:\\work\\vpython\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\work\\vpython\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is (4,)\n",
      "input shape is (4,)\n",
      "Logging to C:\\Users\\cd_bo\\AppData\\Local\\Temp\\openai-2020-12-19-08-13-59-931587\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 95       |\n",
      "| episodes                     | 100      |\n",
      "| mean last 100 episode reward | 22.5     |\n",
      "| steps                        | 2228     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 91       |\n",
      "| episodes                     | 200      |\n",
      "| mean last 100 episode reward | 21.2     |\n",
      "| steps                        | 4348     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 85       |\n",
      "| episodes                     | 300      |\n",
      "| mean last 100 episode reward | 28.7     |\n",
      "| steps                        | 7219     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 81       |\n",
      "| episodes                     | 400      |\n",
      "| mean last 100 episode reward | 20.2     |\n",
      "| steps                        | 9237     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 77       |\n",
      "| episodes                     | 500      |\n",
      "| mean last 100 episode reward | 23.9     |\n",
      "| steps                        | 11630    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 72       |\n",
      "| episodes                     | 600      |\n",
      "| mean last 100 episode reward | 24.1     |\n",
      "| steps                        | 14038    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 67       |\n",
      "| episodes                     | 700      |\n",
      "| mean last 100 episode reward | 26.6     |\n",
      "| steps                        | 16703    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 62       |\n",
      "| episodes                     | 800      |\n",
      "| mean last 100 episode reward | 24       |\n",
      "| steps                        | 19100    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 57       |\n",
      "| episodes                     | 900      |\n",
      "| mean last 100 episode reward | 25.6     |\n",
      "| steps                        | 21657    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 52       |\n",
      "| episodes                     | 1000     |\n",
      "| mean last 100 episode reward | 27.6     |\n",
      "| steps                        | 24412    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 45       |\n",
      "| episodes                     | 1100     |\n",
      "| mean last 100 episode reward | 35.8     |\n",
      "| steps                        | 27997    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 38       |\n",
      "| episodes                     | 1200     |\n",
      "| mean last 100 episode reward | 33       |\n",
      "| steps                        | 31293    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 31       |\n",
      "| episodes                     | 1300     |\n",
      "| mean last 100 episode reward | 38.6     |\n",
      "| steps                        | 35148    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 19       |\n",
      "| episodes                     | 1400     |\n",
      "| mean last 100 episode reward | 59.8     |\n",
      "| steps                        | 41124    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 4        |\n",
      "| episodes                     | 1500     |\n",
      "| mean last 100 episode reward | 74.4     |\n",
      "| steps                        | 48560    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 1600     |\n",
      "| mean last 100 episode reward | 105      |\n",
      "| steps                        | 59017    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 1700     |\n",
      "| mean last 100 episode reward | 95       |\n",
      "| steps                        | 68519    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 1800     |\n",
      "| mean last 100 episode reward | 98.1     |\n",
      "| steps                        | 78327    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 1900     |\n",
      "| mean last 100 episode reward | 85.6     |\n",
      "| steps                        | 86890    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2000     |\n",
      "| mean last 100 episode reward | 100      |\n",
      "| steps                        | 96906    |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2100     |\n",
      "| mean last 100 episode reward | 42.2     |\n",
      "| steps                        | 101124   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2200     |\n",
      "| mean last 100 episode reward | 42.2     |\n",
      "| steps                        | 105344   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2300     |\n",
      "| mean last 100 episode reward | 104      |\n",
      "| steps                        | 115727   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2400     |\n",
      "| mean last 100 episode reward | 97.6     |\n",
      "| steps                        | 125484   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2500     |\n",
      "| mean last 100 episode reward | 104      |\n",
      "| steps                        | 135920   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2600     |\n",
      "| mean last 100 episode reward | 93       |\n",
      "| steps                        | 145221   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2700     |\n",
      "| mean last 100 episode reward | 21.8     |\n",
      "| steps                        | 147396   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2800     |\n",
      "| mean last 100 episode reward | 70.4     |\n",
      "| steps                        | 154439   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 2900     |\n",
      "| mean last 100 episode reward | 80.3     |\n",
      "| steps                        | 162468   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3000     |\n",
      "| mean last 100 episode reward | 29       |\n",
      "| steps                        | 165371   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3100     |\n",
      "| mean last 100 episode reward | 47       |\n",
      "| steps                        | 170068   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3200     |\n",
      "| mean last 100 episode reward | 115      |\n",
      "| steps                        | 181582   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3300     |\n",
      "| mean last 100 episode reward | 108      |\n",
      "| steps                        | 192426   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3400     |\n",
      "| mean last 100 episode reward | 98.6     |\n",
      "| steps                        | 202283   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3500     |\n",
      "| mean last 100 episode reward | 101      |\n",
      "| steps                        | 212362   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3600     |\n",
      "| mean last 100 episode reward | 68.5     |\n",
      "| steps                        | 219216   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3700     |\n",
      "| mean last 100 episode reward | 73.8     |\n",
      "| steps                        | 226598   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3800     |\n",
      "| mean last 100 episode reward | 64.1     |\n",
      "| steps                        | 233009   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 3900     |\n",
      "| mean last 100 episode reward | 52.1     |\n",
      "| steps                        | 238215   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4000     |\n",
      "| mean last 100 episode reward | 27.9     |\n",
      "| steps                        | 241007   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4100     |\n",
      "| mean last 100 episode reward | 48.1     |\n",
      "| steps                        | 245819   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4200     |\n",
      "| mean last 100 episode reward | 41.4     |\n",
      "| steps                        | 249955   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4300     |\n",
      "| mean last 100 episode reward | 110      |\n",
      "| steps                        | 260962   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4400     |\n",
      "| mean last 100 episode reward | 98.2     |\n",
      "| steps                        | 270785   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4500     |\n",
      "| mean last 100 episode reward | 82       |\n",
      "| steps                        | 278987   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4600     |\n",
      "| mean last 100 episode reward | 93.8     |\n",
      "| steps                        | 288370   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4700     |\n",
      "| mean last 100 episode reward | 105      |\n",
      "| steps                        | 298859   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4800     |\n",
      "| mean last 100 episode reward | 107      |\n",
      "| steps                        | 309522   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 4900     |\n",
      "| mean last 100 episode reward | 47.8     |\n",
      "| steps                        | 314305   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5000     |\n",
      "| mean last 100 episode reward | 34       |\n",
      "| steps                        | 317703   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5100     |\n",
      "| mean last 100 episode reward | 39.4     |\n",
      "| steps                        | 321640   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5200     |\n",
      "| mean last 100 episode reward | 73.5     |\n",
      "| steps                        | 328990   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5300     |\n",
      "| mean last 100 episode reward | 80.5     |\n",
      "| steps                        | 337038   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5400     |\n",
      "| mean last 100 episode reward | 67.9     |\n",
      "| steps                        | 343824   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5500     |\n",
      "| mean last 100 episode reward | 79.4     |\n",
      "| steps                        | 351767   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5600     |\n",
      "| mean last 100 episode reward | 112      |\n",
      "| steps                        | 363021   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5700     |\n",
      "| mean last 100 episode reward | 82.6     |\n",
      "| steps                        | 371286   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5800     |\n",
      "| mean last 100 episode reward | 75.1     |\n",
      "| steps                        | 378794   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 5900     |\n",
      "| mean last 100 episode reward | 83.3     |\n",
      "| steps                        | 387122   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6000     |\n",
      "| mean last 100 episode reward | 60.4     |\n",
      "| steps                        | 393159   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6100     |\n",
      "| mean last 100 episode reward | 60.2     |\n",
      "| steps                        | 399180   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6200     |\n",
      "| mean last 100 episode reward | 91.8     |\n",
      "| steps                        | 408365   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6300     |\n",
      "| mean last 100 episode reward | 59       |\n",
      "| steps                        | 414265   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6400     |\n",
      "| mean last 100 episode reward | 31.4     |\n",
      "| steps                        | 417410   |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| % time spent exploring       | 2        |\n",
      "| episodes                     | 6500     |\n",
      "| mean last 100 episode reward | 95       |\n",
      "| steps                        | 426905   |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def callback(locals, globals):\n",
    "    \"\"\"\n",
    "    function called at every step with state of the algorithm.\n",
    "    If callback returns true training stops.\n",
    "    \"\"\"\n",
    "    # stop training if average reward exceeds 199\n",
    "    # time should be greater than 100 and the average of last 100 returns should be >= 199\n",
    "    episode_rewards = locals[\"episode_rewards\"]\n",
    "    is_solved = (\n",
    "        locals[\"t\"] > 100 and sum(episode_rewards[-101:-1]) / 100 >= 195\n",
    "    )\n",
    "    if (is_solved):\n",
    "        logger.record_tabular(\"solved :\")\n",
    "        mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
    "        num_episodes = len(episode_rewards)\n",
    "        logger.record_tabular(\"steps\", locals[\"t\"])\n",
    "        logger.record_tabular(\"episodes\", num_episodes)\n",
    "        logger.record_tabular(\"mean last 100 episode reward\", mean_100ep_reward)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "    return is_solved\n",
    "\n",
    "# create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Prepare learning parameters: network and learning rate\n",
    "# the policy is a multi-layer perceptron\n",
    "network = \"mlp\"\n",
    "# set learning rate of the algorithm\n",
    "learning_rate = 5e-3\n",
    "\n",
    "# launch learning on this environment using DQN\n",
    "# ignore the exploration parameter for now\n",
    "\n",
    "actor = deepq.learn(\n",
    "    env,\n",
    "    network=network,\n",
    "    lr=learning_rate,\n",
    "    total_timesteps=int(5e5),\n",
    "    buffer_size=int(5e4),\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    print_freq=100,\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "print(\"Saving model to cartpole_model.pkl\")\n",
    "save_path = \"cartpole_model.pkl\"\n",
    "ckpt = tf.train.Checkpoint(model=actor)\n",
    "manager = tf.train.CheckpointManager(ckpt, save_path, max_to_keep=None)\n",
    "manager.save()\n",
    "print(\"Saved to cartpole_model.pkl\")\n",
    "#actor.save(\"cartpole_model.pkl\")\n",
    "\n",
    "##%%\n",
    "\n",
    "actor = deepq.learn(\n",
    "    env,\n",
    "    network=network,\n",
    "    lr=learning_rate,\n",
    "    total_timesteps=0,\n",
    "    buffer_size=0,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    print_freq=10,\n",
    "    callback=callback,\n",
    "    load_path=\"cartpole_model.pkl\"\n",
    ")\n",
    "\n",
    "##%%\n",
    "\n",
    "# Needed to show the environment in a notebook\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "from baselines.common.vec_env.vec_env import VecEnv\n",
    "##%%\n",
    "\n",
    "env = wrappers.Monitor(\n",
    "    env, \"./gym-results\", force=True, video_callable=lambda episode_id: True\n",
    ")\n",
    "\n",
    "#visualize the policy\n",
    "n_episodes = 5\n",
    "n_timesteps = 1000\n",
    "for episode in range(n_episodes):\n",
    "    observation = env.reset()\n",
    "    episode_return = 0\n",
    "    for timestep in range(n_timesteps):\n",
    "        # render the environment\n",
    "        env.render()\n",
    "\n",
    "        # select the action according to the actor\n",
    "        if not isinstance(env, VecEnv):\n",
    "            observation = np.expand_dims(np.array(observation), axis=0)\n",
    "        action, _, _, _ = actor.step(tf.constant(observation))\n",
    "        action = action[0].numpy()\n",
    "        #action = actor(observation[None])[0]\n",
    "\n",
    "        # call env.step function\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        # since the reward is undiscounted we can simply add the reward to the cumulated return\n",
    "        episode_return += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    # here an episode is terminated, print the return and the number of steps\n",
    "    print(f\"Episode return {episode_return}, Number of steps: {timestep}\")\n",
    "env.close()\n",
    "\n",
    "##%%\n",
    "\n",
    "# Render the episodes\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "import ffmpy3\n",
    "def render(episodes_to_watch=1):\n",
    "    for episode in range(episodes_to_watch):\n",
    "        videoPath = f\"./gym-results/openaigym.video.{env.file_infix}.video{episode:06d}.mp4\"\n",
    "        videoPathWebm = videoPath.replace(\"mp4\", \"webm\")\n",
    "        ffmpy3.FFmpeg(\n",
    "        inputs={videoPath: None},\n",
    "        outputs={videoPathWebm: None}\n",
    "        ).run()\n",
    "        videoMp4 = io.open(videoPath, \"r+b\").read()\n",
    "        videoWebm = io.open(videoPathWebm, \"r+b\").read()\n",
    "        encodedMp4 = base64.b64encode(videoMp4)\n",
    "        encodedWebm = base64.b64encode(videoWebm)\n",
    "        display(\n",
    "            HTML(\n",
    "                data=\"\"\"<video width=\"360\" height=\"auto\" alt=\"test\" controls autoplay loop><source src=\"data:video/webm;base64,{1}\" type=\"video/webm\" /><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>\"\"\".format(\n",
    "                    encodedMp4.decode(\"ascii\"),encodedWebm.decode(\"ascii\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "render(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (The-Reinforcement-Learning-Workshop-master)",
   "language": "python",
   "name": "pycharm-b67d928d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}