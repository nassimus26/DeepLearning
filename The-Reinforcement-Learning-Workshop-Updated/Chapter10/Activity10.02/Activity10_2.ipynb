{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Activity10_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNcF06Fc7/jF2AHwCaB5u3M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nOFOZNtiLna6","colab_type":"code","colab":{}},"source":["import gym \n","import random\n","import numpy as np\n","from collections import deque\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM\n","from tensorflow.keras.optimizers import RMSprop\n","import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sjNb3YhcyBz","colab_type":"code","colab":{}},"source":["np.random.seed(168)\n","tf.random.set_seed(168)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wfjKpXoLsmg","colab_type":"code","colab":{}},"source":["class DRQN():\n","    def __init__(self, env, batch_size=64, max_experiences=5000):\n","        self.env = env\n","        self.input_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","        self.max_experiences = max_experiences\n","        self.memory = deque(maxlen=self.max_experiences)\n","        self.batch_size = batch_size\n","        self.gamma = 1.0\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        \n","        self.model = self.build_model()\n","        self.target_model = self.build_model()\n","                \n","    def build_model(self):\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv2D(32, 8, (4,4), activation='relu', padding='valid'), input_shape=(SEQUENCE, IMG_SIZE, IMG_SIZE, 1)))\n","        model.add(TimeDistributed(Conv2D(64, 4, (2,2), activation='relu', padding='valid')))\n","        model.add(TimeDistributed(Conv2D(64, 3, (1,1), activation='relu', padding='valid')))\n","        model.add(TimeDistributed(Flatten()))\n","        model.add(LSTM(512))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(self.action_size))\n","        model.compile(loss='mse', optimizer=RMSprop(lr=0.00025, epsilon=self.epsilon_min), metrics=['accuracy'])\n","        return model\n","            \n","    def get_action(self, state):\n","        if np.random.random() <= self.epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            pred = self.model.predict(tf.expand_dims(state, 0))\n","            return np.argmax(pred)\n","\n","    def add_experience(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        self.update_epsilon()\n","\n","    def replay(self, episode):\n","        x_batch, y_batch = [], []\n","        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n","        \n","        for state, action, reward, next_state, done in minibatch:\n","            y_target = self.target_model.predict(tf.expand_dims(state, 0))\n","            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(tf.expand_dims(next_state, 0))[0])\n","            x_batch.append(state)\n","            y_batch.append(y_target[0])\n","\n","        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)   \n","\n","    def update_epsilon(self):\n","        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDwTwc7VPM0F","colab_type":"code","colab":{}},"source":["def initialize_env(env):\n","  initial_state = env.reset()\n","  initial_done_flag = False\n","  initial_rewards = 0\n","  return initial_state, initial_done_flag, initial_rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xj9Qq-n4I7_w","colab_type":"code","colab":{}},"source":["def preprocess_state(image, img_size):\n","    img_temp = image[31:195]\n","    img_temp = tf.image.rgb_to_grayscale(img_temp)\n","    img_temp = tf.image.resize(img_temp, [img_size, img_size],\n","                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    img_temp = tf.cast(img_temp, tf.float32)\n","    return img_temp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n0s7dHpI8C6","colab_type":"code","colab":{}},"source":["def combine_images(new_img, prev_img, img_size, seq=4):\n","    if len(prev_img.shape) == 4 and prev_img.shape[0] == seq:\n","        im = np.concatenate((prev_img[1:, :, :], tf.reshape(new_img, [1, img_size, img_size, 1])), axis=0)\n","    else:\n","        im = np.stack([new_img] * seq, axis=0)\n","    return im"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZbzN_AZPPdx","colab_type":"code","colab":{}},"source":["def play_game(agent, state, done, rewards):    \n","    while not done:\n","        action = agent.get_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","\n","        next_state = preprocess_state(next_state, IMG_SIZE)\n","        next_state = combine_images(new_img=next_state, prev_img=state, img_size=IMG_SIZE, seq=SEQUENCE)\n","        \n","        agent.add_experience(state, action, reward, next_state, done)\n","\n","        state = next_state\n","        rewards += reward   \n","    return rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axc6Pa3xPvn6","colab_type":"code","colab":{}},"source":["def train_agent(env, episodes, agent):\n","  from collections import deque\n","  import numpy as np\n","\n","  scores = deque(maxlen=100)\n","\n","  for episode in range(episodes):\n","    state, done, rewards = initialize_env(env) \n","    state = preprocess_state(state, IMG_SIZE)\n","    state = combine_images(new_img=state, prev_img=state, img_size=IMG_SIZE, seq=SEQUENCE)\n","\n","    rewards = play_game(agent, state, done, rewards)\n","    scores.append(rewards)\n","    mean_score = np.mean(scores)\n","\n","    if episode % 50 == 0:\n","        print(f'[Episode {episode}] - Average Score: {mean_score}')\n","        agent.target_model.set_weights(agent.model.get_weights())\n","        agent.target_model.save_weights(f'drqn/drqn_model_weights_{episode}')\n","\n","    agent.replay(episode)\n","\n","  print(f\"Average Score: {np.mean(scores)}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yc3Iz-AcPxyG","colab_type":"code","outputId":"a9693375-6055-48da-c0d6-b8ebcbd5cd4f","executionInfo":{"status":"ok","timestamp":1588608013250,"user_tz":-600,"elapsed":13711859,"user":{"displayName":"Anthony So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYepcJQjaVrQ1i01LEROgsJ738vi03JrR51Ryb3w=s64","userId":"11809607246124237079"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["env = gym.make('BreakoutDeterministic-v4')\n","IMG_SIZE = 84\n","SEQUENCE = 4\n","agent = DRQN(env)\n","episodes = 200\n","train_agent(env, episodes, agent)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[Episode 0] - Average Score: 0.0\n","[Episode 50] - Average Score: 0.43137254901960786\n","[Episode 100] - Average Score: 0.4\n","[Episode 150] - Average Score: 0.54\n","Average Score: 0.53\n"],"name":"stdout"}]}]}
